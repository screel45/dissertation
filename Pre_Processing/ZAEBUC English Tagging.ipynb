{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Anon_ID\t19632\n",
      "\n",
      "How do social media affect individuals and society?\n",
      "\n",
      "100 years ago all these technology was not found yet, people use to go and visit each other most of the time either they by camels or horses or even by walking for miles. Most people use to communicate by visiting each other and that's how most of the news used to get explored all other the city.\n",
      "\n",
      "social media can be any app that you can communicate or share any picture or video by using the Wi-Fi or, let's say 3G. there are many ways that people might use social media in some can be positive and some can be negative. sadly day by day social media is getting worse they keep using it in a negative way. Let's say some people abuse girls and boys from different ages. although people started to get famous on Instagram and snapchat in a way that they keep hurting others - let's say sharing embarrassing pictures of famous people or telling lies about them. Most important is the way the person uses this app because he is the one to give instructions, not the App. People must start thinking wisely and take extra care of people's feelings because that might lead others to death.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "english_essay = []\n",
    "with open(\"/Users/samanthacreel/Documents/Dissertation/ZAEBUC_spellcorrected/English_spellcorrected_30-1-2021/EN-140-19632.extracted.spellcorrected.txt\") as f:\n",
    "    english_essay = f.read()#.splitlines()\n",
    "print(english_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/samanthacreel/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1711a356d90f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStanfordTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0messay_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_essay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0messay_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \"\"\"\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     return [\n\u001b[1;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/samanthacreel/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.8/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import StanfordTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/samanthacreel/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'Anon_ID', '19632', 'How', 'do', 'social', 'media', 'affect', 'individuals', 'and', 'society', '?', '100', 'years', 'ago', 'all', 'these', 'technology', 'was', 'not', 'found', 'yet', ',', 'people', 'use', 'to', 'go', 'and', 'visit', 'each', 'other', 'most', 'of', 'the', 'time', 'either', 'they', 'by', 'camels', 'or', 'horses', 'or', 'even', 'by', 'walking', 'for', 'miles', '.', 'Most', 'people', 'use', 'to', 'communicate', 'by', 'visiting', 'each', 'other', 'and', 'that', \"'s\", 'how', 'most', 'of', 'the', 'news', 'used', 'to', 'get', 'explored', 'all', 'other', 'the', 'city', '.', 'social', 'media', 'can', 'be', 'any', 'app', 'that', 'you', 'can', 'communicate', 'or', 'share', 'any', 'picture', 'or', 'video', 'by', 'using', 'the', 'Wi-Fi', 'or', ',', 'let', \"'s\", 'say', '3G', '.', 'there', 'are', 'many', 'ways', 'that', 'people', 'might', 'use', 'social', 'media', 'in', 'some', 'can', 'be', 'positive', 'and', 'some', 'can', 'be', 'negative', '.', 'sadly', 'day', 'by', 'day', 'social', 'media', 'is', 'getting', 'worse', 'they', 'keep', 'using', 'it', 'in', 'a', 'negative', 'way', '.', 'Let', \"'s\", 'say', 'some', 'people', 'abuse', 'girls', 'and', 'boys', 'from', 'different', 'ages', '.', 'although', 'people', 'started', 'to', 'get', 'famous', 'on', 'Instagram', 'and', 'snapchat', 'in', 'a', 'way', 'that', 'they', 'keep', 'hurting', 'others', '-', 'let', \"'s\", 'say', 'sharing', 'embarrassing', 'pictures', 'of', 'famous', 'people', 'or', 'telling', 'lies', 'about', 'them', '.', 'Most', 'important', 'is', 'the', 'way', 'the', 'person', 'uses', 'this', 'app', 'because', 'he', 'is', 'the', 'one', 'to', 'give', 'instructions', ',', 'not', 'the', 'App', '.', 'People', 'must', 'start', 'thinking', 'wisely', 'and', 'take', 'extra', 'care', 'of', 'people', \"'s\", 'feelings', 'because', 'that', 'might', 'lead', 'others', 'to', 'death', '.']\n"
     ]
    }
   ],
   "source": [
    "essay_tok = nltk.word_tokenize(english_essay)\n",
    "print(essay_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', '#'), ('Anon_ID', 'NNP'), ('19632', 'CD'), ('How', 'NNP'), ('do', 'VBP'), ('social', 'JJ'), ('media', 'NNS'), ('affect', 'JJ'), ('individuals', 'NNS'), ('and', 'CC'), ('society', 'NN'), ('?', '.'), ('100', 'CD'), ('years', 'NNS'), ('ago', 'RB'), ('all', 'PDT'), ('these', 'DT'), ('technology', 'NN'), ('was', 'VBD'), ('not', 'RB'), ('found', 'VBN'), ('yet', 'RB'), (',', ','), ('people', 'NNS'), ('use', 'VBP'), ('to', 'TO'), ('go', 'VB'), ('and', 'CC'), ('visit', 'VB'), ('each', 'DT'), ('other', 'JJ'), ('most', 'RBS'), ('of', 'IN'), ('the', 'DT'), ('time', 'NN'), ('either', 'CC'), ('they', 'PRP'), ('by', 'IN'), ('camels', 'NNS'), ('or', 'CC'), ('horses', 'NNS'), ('or', 'CC'), ('even', 'RB'), ('by', 'IN'), ('walking', 'VBG'), ('for', 'IN'), ('miles', 'NNS'), ('.', '.'), ('Most', 'JJS'), ('people', 'NNS'), ('use', 'VBP'), ('to', 'TO'), ('communicate', 'VB'), ('by', 'IN'), ('visiting', 'VBG'), ('each', 'DT'), ('other', 'JJ'), ('and', 'CC'), ('that', 'DT'), (\"'s\", 'VBZ'), ('how', 'WRB'), ('most', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('news', 'NN'), ('used', 'VBN'), ('to', 'TO'), ('get', 'VB'), ('explored', 'VBN'), ('all', 'DT'), ('other', 'JJ'), ('the', 'DT'), ('city', 'NN'), ('.', '.'), ('social', 'JJ'), ('media', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('any', 'DT'), ('app', 'NN'), ('that', 'IN'), ('you', 'PRP'), ('can', 'MD'), ('communicate', 'VB'), ('or', 'CC'), ('share', 'NN'), ('any', 'DT'), ('picture', 'NN'), ('or', 'CC'), ('video', 'NN'), ('by', 'IN'), ('using', 'VBG'), ('the', 'DT'), ('Wi-Fi', 'NNP'), ('or', 'CC'), (',', ','), ('let', 'VB'), (\"'s\", 'POS'), ('say', 'VB'), ('3G', 'CD'), ('.', '.'), ('there', 'EX'), ('are', 'VBP'), ('many', 'JJ'), ('ways', 'NNS'), ('that', 'WDT'), ('people', 'NNS'), ('might', 'MD'), ('use', 'VB'), ('social', 'JJ'), ('media', 'NNS'), ('in', 'IN'), ('some', 'DT'), ('can', 'MD'), ('be', 'VB'), ('positive', 'JJ'), ('and', 'CC'), ('some', 'DT'), ('can', 'MD'), ('be', 'VB'), ('negative', 'JJ'), ('.', '.'), ('sadly', 'RB'), ('day', 'NN'), ('by', 'IN'), ('day', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('is', 'VBZ'), ('getting', 'VBG'), ('worse', 'RBR'), ('they', 'PRP'), ('keep', 'VBP'), ('using', 'VBG'), ('it', 'PRP'), ('in', 'IN'), ('a', 'DT'), ('negative', 'JJ'), ('way', 'NN'), ('.', '.'), ('Let', 'VB'), (\"'s\", 'POS'), ('say', 'VB'), ('some', 'DT'), ('people', 'NNS'), ('abuse', 'VBP'), ('girls', 'NNS'), ('and', 'CC'), ('boys', 'NNS'), ('from', 'IN'), ('different', 'JJ'), ('ages', 'NNS'), ('.', '.'), ('although', 'IN'), ('people', 'NNS'), ('started', 'VBD'), ('to', 'TO'), ('get', 'VB'), ('famous', 'JJ'), ('on', 'IN'), ('Instagram', 'NNP'), ('and', 'CC'), ('snapchat', 'RB'), ('in', 'IN'), ('a', 'DT'), ('way', 'NN'), ('that', 'IN'), ('they', 'PRP'), ('keep', 'VBP'), ('hurting', 'VBG'), ('others', 'NNS'), ('-', ':'), ('let', 'NN'), (\"'s\", 'POS'), ('say', 'VB'), ('sharing', 'VBG'), ('embarrassing', 'VBG'), ('pictures', 'NNS'), ('of', 'IN'), ('famous', 'JJ'), ('people', 'NNS'), ('or', 'CC'), ('telling', 'VBG'), ('lies', 'NNS'), ('about', 'IN'), ('them', 'PRP'), ('.', '.'), ('Most', 'RBS'), ('important', 'JJ'), ('is', 'VBZ'), ('the', 'DT'), ('way', 'NN'), ('the', 'DT'), ('person', 'NN'), ('uses', 'VBZ'), ('this', 'DT'), ('app', 'NN'), ('because', 'IN'), ('he', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('one', 'NN'), ('to', 'TO'), ('give', 'VB'), ('instructions', 'NNS'), (',', ','), ('not', 'RB'), ('the', 'DT'), ('App', 'NNP'), ('.', '.'), ('People', 'NNS'), ('must', 'MD'), ('start', 'VB'), ('thinking', 'VBG'), ('wisely', 'RB'), ('and', 'CC'), ('take', 'VB'), ('extra', 'JJ'), ('care', 'NN'), ('of', 'IN'), ('people', 'NNS'), (\"'s\", 'POS'), ('feelings', 'NNS'), ('because', 'IN'), ('that', 'DT'), ('might', 'MD'), ('lead', 'VB'), ('others', 'NNS'), ('to', 'TO'), ('death', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "essaypos_tagged = nltk.pos_tag(essay_tok)\n",
    "print(essaypos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#/#\n",
      "Anon_ID/NNP\n",
      "19632/CD\n",
      "How/NNP\n",
      "do/VBP\n",
      "social/JJ\n",
      "media/NNS\n",
      "affect/JJ\n",
      "individuals/NNS\n",
      "and/CC\n",
      "society/NN\n",
      "?/.\n",
      "100/CD\n",
      "years/NNS\n",
      "ago/RB\n",
      "all/PDT\n",
      "these/DT\n",
      "technology/NN\n",
      "was/VBD\n",
      "not/RB\n",
      "found/VBN\n",
      "yet/RB\n",
      ",/,\n",
      "people/NNS\n",
      "use/VBP\n",
      "to/TO\n",
      "go/VB\n",
      "and/CC\n",
      "visit/VB\n",
      "each/DT\n",
      "other/JJ\n",
      "most/RBS\n",
      "of/IN\n",
      "the/DT\n",
      "time/NN\n",
      "either/CC\n",
      "they/PRP\n",
      "by/IN\n",
      "camels/NNS\n",
      "or/CC\n",
      "horses/NNS\n",
      "or/CC\n",
      "even/RB\n",
      "by/IN\n",
      "walking/VBG\n",
      "for/IN\n",
      "miles/NNS\n",
      "./.\n",
      "Most/JJS\n",
      "people/NNS\n",
      "use/VBP\n",
      "to/TO\n",
      "communicate/VB\n",
      "by/IN\n",
      "visiting/VBG\n",
      "each/DT\n",
      "other/JJ\n",
      "and/CC\n",
      "that/DT\n",
      "'s/VBZ\n",
      "how/WRB\n",
      "most/JJS\n",
      "of/IN\n",
      "the/DT\n",
      "news/NN\n",
      "used/VBN\n",
      "to/TO\n",
      "get/VB\n",
      "explored/VBN\n",
      "all/DT\n",
      "other/JJ\n",
      "the/DT\n",
      "city/NN\n",
      "./.\n",
      "social/JJ\n",
      "media/NNS\n",
      "can/MD\n",
      "be/VB\n",
      "any/DT\n",
      "app/NN\n",
      "that/IN\n",
      "you/PRP\n",
      "can/MD\n",
      "communicate/VB\n",
      "or/CC\n",
      "share/NN\n",
      "any/DT\n",
      "picture/NN\n",
      "or/CC\n",
      "video/NN\n",
      "by/IN\n",
      "using/VBG\n",
      "the/DT\n",
      "Wi-Fi/NNP\n",
      "or/CC\n",
      ",/,\n",
      "let/VB\n",
      "'s/POS\n",
      "say/VB\n",
      "3G/CD\n",
      "./.\n",
      "there/EX\n",
      "are/VBP\n",
      "many/JJ\n",
      "ways/NNS\n",
      "that/WDT\n",
      "people/NNS\n",
      "might/MD\n",
      "use/VB\n",
      "social/JJ\n",
      "media/NNS\n",
      "in/IN\n",
      "some/DT\n",
      "can/MD\n",
      "be/VB\n",
      "positive/JJ\n",
      "and/CC\n",
      "some/DT\n",
      "can/MD\n",
      "be/VB\n",
      "negative/JJ\n",
      "./.\n",
      "sadly/RB\n",
      "day/NN\n",
      "by/IN\n",
      "day/NN\n",
      "social/JJ\n",
      "media/NNS\n",
      "is/VBZ\n",
      "getting/VBG\n",
      "worse/RBR\n",
      "they/PRP\n",
      "keep/VBP\n",
      "using/VBG\n",
      "it/PRP\n",
      "in/IN\n",
      "a/DT\n",
      "negative/JJ\n",
      "way/NN\n",
      "./.\n",
      "Let/VB\n",
      "'s/POS\n",
      "say/VB\n",
      "some/DT\n",
      "people/NNS\n",
      "abuse/VBP\n",
      "girls/NNS\n",
      "and/CC\n",
      "boys/NNS\n",
      "from/IN\n",
      "different/JJ\n",
      "ages/NNS\n",
      "./.\n",
      "although/IN\n",
      "people/NNS\n",
      "started/VBD\n",
      "to/TO\n",
      "get/VB\n",
      "famous/JJ\n",
      "on/IN\n",
      "Instagram/NNP\n",
      "and/CC\n",
      "snapchat/RB\n",
      "in/IN\n",
      "a/DT\n",
      "way/NN\n",
      "that/IN\n",
      "they/PRP\n",
      "keep/VBP\n",
      "hurting/VBG\n",
      "others/NNS\n",
      "-/:\n",
      "let/NN\n",
      "'s/POS\n",
      "say/VB\n",
      "sharing/VBG\n",
      "embarrassing/VBG\n",
      "pictures/NNS\n",
      "of/IN\n",
      "famous/JJ\n",
      "people/NNS\n",
      "or/CC\n",
      "telling/VBG\n",
      "lies/NNS\n",
      "about/IN\n",
      "them/PRP\n",
      "./.\n",
      "Most/RBS\n",
      "important/JJ\n",
      "is/VBZ\n",
      "the/DT\n",
      "way/NN\n",
      "the/DT\n",
      "person/NN\n",
      "uses/VBZ\n",
      "this/DT\n",
      "app/NN\n",
      "because/IN\n",
      "he/PRP\n",
      "is/VBZ\n",
      "the/DT\n",
      "one/NN\n",
      "to/TO\n",
      "give/VB\n",
      "instructions/NNS\n",
      ",/,\n",
      "not/RB\n",
      "the/DT\n",
      "App/NNP\n",
      "./.\n",
      "People/NNS\n",
      "must/MD\n",
      "start/VB\n",
      "thinking/VBG\n",
      "wisely/RB\n",
      "and/CC\n",
      "take/VB\n",
      "extra/JJ\n",
      "care/NN\n",
      "of/IN\n",
      "people/NNS\n",
      "'s/POS\n",
      "feelings/NNS\n",
      "because/IN\n",
      "that/DT\n",
      "might/MD\n",
      "lead/VB\n",
      "others/NNS\n",
      "to/TO\n",
      "death/NN\n",
      "./.\n"
     ]
    }
   ],
   "source": [
    "for word,word_class in essaypos_tagged:\n",
    "    print(word + \"/\" + word_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "folderpath = \"/Users/samanthacreel/Documents/Dissertation/ZAEBUC_spellcorrected/English_spellcorrected_30-1-2021\"\n",
    "filenames = os.listdir(folderpath)\n",
    "filenames = [file for file in filenames if not file.startswith('.')]\n",
    "outputfolder = \"/Users/samanthacreel/Documents/Dissertation/ZAEBUC_spellcorrected/ZAEBUC_English_tagged\"\n",
    "\n",
    "for file in filenames:\n",
    "    with open(os.path.join(folderpath,file),'r') as f:\n",
    "        english_essay = f.read()\n",
    "        essay_tok = nltk.word_tokenize(english_essay)\n",
    "        essaypos_tagged = nltk.pos_tag(essay_tok)\n",
    "        taggedessay = []\n",
    "        for word,word_class in essaypos_tagged:\n",
    "            newitem = word + \"/\" + word_class\n",
    "            taggedessay.append(newitem)\n",
    "        with open(os.path.join(outputfolder, str(file) + \"_POSTagged.txt\"), \"w\") as outf:\n",
    "            for item in taggedessay:\n",
    "                outf.write(item + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to UTF 16\n",
    "essay1 = \"/Users/samanthacreel/Documents/Dissertation/ZAEBUC_spellcorrected/ZAEBUC_English_tagged/EN-140-19632.extracted.spellcorrected.txt_POSTagged.txt\"\n",
    "outessay = \"/Users/samanthacreel/Documents/Dissertation/ZAEBUC_spellcorrected/ZAEBUC_English_tagged_UTF16/essay1.txt\"\n",
    "with open(essay1, 'rb') as source_file:\n",
    "    with open(outessay, 'w+b') as dest_file:\n",
    "        contents = source_file.read()\n",
    "        dest_file.write(contents.decode('utf-8').encode('utf-16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e8da7b31aadf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputfolder_tagged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_POSTagged_UTF16.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0moutf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-16'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "folderpath_tagged = \"/Users/samanthacreel/Documents/Dissertation/ZAEBUC_spellcorrected/ZAEBUC_English_tagged\"\n",
    "filenames_tagged = os.listdir(folderpath_tagged)\n",
    "filenames_tagged = [file for file in filenames_tagged if not file.startswith('.')]\n",
    "outputfolder_tagged = \"/Users/samanthacreel/Documents/Dissertation/ZAEBUC_spellcorrected/ZAEBUC_English_tagged_UTF16\"\n",
    "\n",
    "for file in filenames_tagged:\n",
    "    with open(os.path.join(folderpath_tagged,file),'rb') as f:\n",
    "        with open(os.path.join(outputfolder_tagged, str(file) + \"_POSTagged_UTF16.txt\"), \"w+b\") as outf:\n",
    "            contents = f.read()\n",
    "            outf.write(contents.decode('utf-8').encode('utf-16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
